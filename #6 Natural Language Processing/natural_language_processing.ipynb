{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VwK5-9FIB-lu"},"source":["# Natural Language Processing"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"X1kiO9kACE6s"},"source":["## Importing the libraries"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wTfaCIzdCLPA"},"source":["## Importing the dataset"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["dataset = pd.read_csv('Data/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Qekztq71CixT"},"source":["## Cleaning the texts"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\jadha\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import re\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","corpus = []\n","for i in range(0, 1000):\n","    review = re.sub('[^a-zA-z]', ' ', dataset['Review'][i])\n","    review = review.lower()\n","    review = review.split()\n","    ps = PorterStemmer()\n","    all_stopwords = stopwords.words('english')\n","    all_stopwords.remove('not')\n","    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n","    review = ' '.join(review)\n","    corpus.append(review)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CLqmAkANCp1-"},"source":["## Creating the Bag of Words model"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer(max_features = 1500)\n","x = cv.fit_transform(corpus).toarray()\n","y = dataset.iloc[:, -1].values"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DH_VjgPzC2cd"},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VkIq23vEDIPt"},"source":["## Training the Naive Bayes model on the Training set"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["SVC(random_state=0)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.svm import SVC\n","classifier = SVC(kernel = 'rbf', random_state = 0, degree = 3)\n","classifier.fit(x_train, y_train)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1JaRM7zXDWUy"},"source":["## Predicting the Test set results"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 0]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [1 1]\n"," [1 1]\n"," [1 1]\n"," [1 0]\n"," [1 1]\n"," [1 1]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [1 1]\n"," [0 0]\n"," [0 1]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [1 1]\n"," [0 1]\n"," [1 1]\n"," [1 1]\n"," [0 0]\n"," [0 1]\n"," [0 1]\n"," [0 1]\n"," [0 1]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [1 1]\n"," [1 1]\n"," [0 0]\n"," [0 1]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [1 0]\n"," [1 0]\n"," [0 0]\n"," [0 0]\n"," [1 1]\n"," [0 1]\n"," [1 1]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [0 1]\n"," [0 1]\n"," [0 0]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [1 0]\n"," [0 1]\n"," [0 0]\n"," [1 1]\n"," [0 1]\n"," [0 1]\n"," [0 0]\n"," [1 1]\n"," [1 1]\n"," [1 1]\n"," [0 1]\n"," [0 0]\n"," [0 0]\n"," [0 1]\n"," [1 1]\n"," [0 0]\n"," [1 1]\n"," [0 0]\n"," [1 1]\n"," [1 1]\n"," [0 0]\n"," [1 1]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [0 1]\n"," [1 0]\n"," [0 1]\n"," [0 1]\n"," [0 0]\n"," [0 1]\n"," [1 1]\n"," [1 1]\n"," [1 0]\n"," [0 1]\n"," [0 0]\n"," [1 1]\n"," [1 1]\n"," [0 0]\n"," [0 1]\n"," [0 1]\n"," [1 1]\n"," [0 0]\n"," [1 0]\n"," [0 1]\n"," [0 0]\n"," [1 1]\n"," [1 1]\n"," [1 1]\n"," [1 1]\n"," [0 1]\n"," [0 0]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [0 1]\n"," [0 0]\n"," [0 0]\n"," [0 1]\n"," [0 0]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [1 1]\n"," [1 1]\n"," [1 1]\n"," [1 1]\n"," [1 1]\n"," [0 0]\n"," [1 1]\n"," [1 1]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [0 1]\n"," [0 1]\n"," [0 1]\n"," [0 1]\n"," [0 1]\n"," [1 1]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [1 1]\n"," [0 1]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [1 1]\n"," [1 1]\n"," [1 0]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [0 1]\n"," [0 0]\n"," [1 1]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [0 1]\n"," [0 0]\n"," [1 1]\n"," [0 0]\n"," [1 1]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [0 1]\n"," [0 0]\n"," [1 1]\n"," [0 0]\n"," [1 1]\n"," [1 1]\n"," [0 0]\n"," [0 0]\n"," [0 0]\n"," [1 1]\n"," [0 0]\n"," [1 1]\n"," [1 1]\n"," [0 0]\n"," [0 1]]\n"]}],"source":["y_pred = classifier.predict(x_test)\n","print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_pred), 1)), 1))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xoMltea5Dir1"},"source":["## Making the Confusion Matrix"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.78\n","[[89  8]\n"," [36 67]]\n"]}],"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","print(accuracy_score(y_test, y_pred))\n","print(confusion_matrix(y_test, y_pred))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMx/KsxUDrn2M5QbIb03B9p","collapsed_sections":[],"name":"natural_language_processing.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"}},"nbformat":4,"nbformat_minor":0}
